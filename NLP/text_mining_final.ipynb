{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application à la classification : l’analyse d’opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import SnowballStemmer\n",
    "from nltk import pos_tag\n",
    "# from collections import namedtuple\n",
    "# CondProb = namedtuple('CondProb', 'prior_idx likelihood_idx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I Implementation du classifieur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Compléter la fonction count_words qui va compter le nombre d’occurrences de chaque mot dans une liste de string et renvoyer le vocabulaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset\n",
      "2000 documents\n",
      "0.823\n"
     ]
    }
   ],
   "source": [
    "# %load sentimentanalysis.py\n",
    "# Authors: Alexandre Gramfort\n",
    "#          Chloe Clavel\n",
    "# License: BSD Style.\n",
    "# TP Cours ML Telecom ParisTech MDI343\n",
    "\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "###############################################################################\n",
    "# Load data\n",
    "print(\"Loading dataset\")\n",
    "\n",
    "from glob import glob\n",
    "filenames_neg = sorted(glob(op.join('..', 'data', 'imdb1', 'neg', '*.txt')))\n",
    "filenames_pos = sorted(glob(op.join('..', 'data', 'imdb1', 'pos', '*.txt')))\n",
    "\n",
    "texts_neg = [open(f).read() for f in filenames_neg]\n",
    "texts_pos = [open(f).read() for f in filenames_pos]\n",
    "texts = texts_neg + texts_pos\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))\n",
    "\n",
    "###############################################################################\n",
    "# Start part to fill in\n",
    "\n",
    "\n",
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    # first iteration: know number of words to initialize array\n",
    "    n_samples = len(texts)\n",
    "    # second version: use strip and lower to minimize vocabulary\n",
    "    words = set(word.strip().lower() for text in texts\n",
    "                for word in text.split())\n",
    "    word_idx = dict((word, id_w) for id_w, word in enumerate(words))\n",
    "    n_features = len(words)\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "\n",
    "    # second iteration: word count\n",
    "    # parsing all texts\n",
    "    vocabulary = dict((word, word_idx[word]) for word in words)\n",
    "    for id_text, text in enumerate(texts):\n",
    "        # parsing a text\n",
    "        for word in text.split():\n",
    "            # evaluate a word\n",
    "            word_clean = word.strip().lower()\n",
    "            counts[id_text][word_idx[word_clean]] = \\\n",
    "                counts[id_text][word_idx[word_clean]] + 1\n",
    "    # counts = np.zeros((len(texts, n_features)))\n",
    "    return vocabulary, counts\n",
    "\n",
    "\n",
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, D):\n",
    "        # get word dictionary\n",
    "        self.D = D\n",
    "        # label => array index\n",
    "        self.label_idx = dict()\n",
    "        # array index => label (reverse dictionary)\n",
    "        self.index2label = dict()\n",
    "        self.likelihood = None\n",
    "        self.prior = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # class index dictionary:\n",
    "        # class 0 -> indexes, class 1 -> indexes\n",
    "        labels = set(np.unique(y))\n",
    "        d_indexes = {label: np.where(y == label)[0]\n",
    "                     for label in labels}\n",
    "        # total samples count\n",
    "        N = X.shape[0]\n",
    "        # label indexing\n",
    "        self.label_idx = dict((label, id_label)\n",
    "                              for id_label, label in enumerate(labels))\n",
    "        self.index2label = dict((self.label_idx[label], label)\n",
    "                                for label in self.label_idx)\n",
    "        # initialize likelihood array (n_labels x p words)\n",
    "        self.likelihood = np.zeros((len(labels), X.shape[1]))\n",
    "        # initialize prior array (n labels)\n",
    "        self.prior = np.zeros(len(labels))\n",
    "        # count all training tokens\n",
    "        # countAllTokens = np.sum(X, axis=0)\n",
    "        for label in d_indexes:\n",
    "            # Compute prior\n",
    "            self.prior[self.label_idx[label]] = d_indexes[0].shape[0] / N\n",
    "            # print(self.prior)\n",
    "            # Compute sum of each word for each class\n",
    "            countTokensFromDoc = np.sum(X[d_indexes[label]], axis=0) + 1\n",
    "            countAllTokens = np.asscalar(np.sum(countTokensFromDoc))\n",
    "            # print(countTokensFromDoc)\n",
    "            # Compute probability given class with Laplace smoothing\n",
    "            self.likelihood[self.label_idx[label]] = \\\n",
    "                countTokensFromDoc / countAllTokens\n",
    "            # print(self.likelihood)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # compute the score of for each row of X for each class\n",
    "        # #words within test file are already in matrix\n",
    "        # product of word probabilities is then equivalent\n",
    "        # to matrix multiplication between X[text] and log(parameters)\n",
    "        prediction = X @ np.log(self.likelihood.T)\n",
    "        # add the prior log\n",
    "        prediction += np.log(self.prior)\n",
    "        # print(prediction)\n",
    "\n",
    "        # 1 vs all\n",
    "        majority_class = np.argmax(prediction, axis=1).astype(np.int)\n",
    "\n",
    "        # convert index -> labels\n",
    "        return np.array([self.index2label[index] for index in majority_class])\n",
    "\n",
    "        # return (np.random.randn(len(X)) > 0).astype(np.int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "\n",
    "# Count words in text\n",
    "vocabulary, X = count_words(texts)\n",
    "\n",
    "# Try to fit, predict and score\n",
    "nb = NB(vocabulary)\n",
    "nb.fit(X[::2], y[::2])\n",
    "print(nb.score(X[1::2], y[1::2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size : 50920\n",
      "#samples : 2000; #words : 50920\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size : {}\".format(len(vocabulary)))\n",
    "print(\"#samples : {}; #words : {}\".format(X.shape[0], X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Expliquer comment les classes positives et négatives ont été assignées sur les critiques de films (voir fichier poldata.README.2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les commentaires avaient été préalablement tagués sur IMDB ainsi que sur les autres sources: lorsqu'un internaute emmet un commentaire, il doit également lui adjoindre une note entre 1 et 5 étoiles sur IMDB. \n",
    "- Une note clairement positive (4 ou 5 etoiles / 5) est considérée comme un avis positif\n",
    "- Une note clairement négative (1 ou 2 étoiles / 5) est considérée comme un avis négatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Compléter la classe NB pour qu’elle implémente le classifieur Naive Bayes en vous appuyant sur le pseudo-code de la figure 1 et sa documentation ci-dessous :\n",
    "- le vocabulaire V correspond à l’ensemble des mots différents composant un ensemble de documents (vocabulary dans count_words)\n",
    "- C correspond à l’ensemble des classes et D l’ensemble des documents\n",
    "- La fonction countTokensOfTerm(text,t) représente le nombre d’occurrences d’un mot t dans un ensemble de textes text (calcul fait dans count_words)\n",
    "- L’étape de lissage appelée lissage de Laplace (+1 ligne 10) permet l’attribution de probabilité non nulle à des mots qui n’interviendraient pas dans l’ensemble d’apprentissage\n",
    "- La fonction ExtractTokensFromDoc(V,d) récupère le vocabulaire associé au document d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe est complétée directement dans le source sentimentanalysis.py.\n",
    "En pratique:\n",
    "- J instancie la classe avec un dictionnaire contenant l'ensemble des mots à prendre en compte (self.D), ainsi que leur index dans le ndarray\n",
    "- La fonction fit effectue l'étape de training:\n",
    " - On récupère l'ensemble des classes de y (labels)\n",
    " - On compte l'ensemble des samples (N)\n",
    " - Les mots pris en compte sont ceux du dictionnaire (D)\n",
    " - On crée une matrice contenant les probabilités d'apparition de chaque mot du dictionnaire dans une classe (len(labels) x len(D))\n",
    " - Pour chaque label, on calcule le prior (p(classe)) ainsi que la fréquence d'apparition de chaque mot dans chaque classe en effectuant un lissage de Laplace sur la probabilité\n",
    "- La fonction test calcule les prédictions de labels sur l ensemble de test:\n",
    " - On se ramène à un produit matriciel pour le calcul du posterior car on passe par le logarithme du prior et des probabilités conditionnelles (on assume indépendantes)\n",
    " - La classe prédite est la classe dont le posterior est le plus grand\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Evaluer les performances de votre classifieur en cross-validation 5-folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81  ,  0.83  ,  0.8175,  0.825 ,  0.795 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = NB(vocabulary)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance du prédicteur sur le test set est de l'ordre de 80%. Après cross validation, la meilleure valeur est à 83%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "Modifiez la fonction count_words pour qu’elle ignore les “stop words” dans le fichier\n",
    "data/english.stop. Les performances sont-elles améliorées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw_file = \"../data/english.stop\"\n",
    "stop_words = set()\n",
    "\n",
    "with open(sw_file) as sw_fileReader:\n",
    "    for line in sw_fileReader:\n",
    "        stop_words.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words_2(texts, stop_words):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "        n_samples == number of documents.\n",
    "        n_features == number of words in vocabulary.\n",
    "    \"\"\"\n",
    "    # first iteration: know number of words to initialize array\n",
    "    n_samples = len(texts)\n",
    "    # second version: use strip and lower to minimize vocabulary\n",
    "    # third version: remove stop words\n",
    "    words = set(word.strip().lower() for text in texts\n",
    "                for word in text.split()) - stop_words\n",
    "    word_idx = dict((word, id_w) for id_w, word in enumerate(words))\n",
    "    n_features = len(words)\n",
    "    counts = np.zeros((n_samples, n_features))\n",
    "\n",
    "    # second iteration: word count\n",
    "    # parsing all texts\n",
    "    vocabulary = dict((word, word_idx[word]) for word in words)\n",
    "    for id_text, text in enumerate(texts):\n",
    "        # parsing a text\n",
    "        for word in text.split():\n",
    "            # evaluate a word\n",
    "            word_clean = word.strip().lower()\n",
    "            if word_clean not in stop_words:\n",
    "                counts[id_text][word_idx[word_clean]] = \\\n",
    "                    counts[id_text][word_idx[word_clean]] + 1\n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples : 2000; #words : 50375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.8025,  0.8125,  0.8125,  0.825 ,  0.785 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count words in text\n",
    "vocabulary_2, X_2 = count_words_2(texts, stop_words)\n",
    "\n",
    "print(\"#samples : {}; #words : {}\".format(X_2.shape[0], X_2.shape[1]))\n",
    "\n",
    "# Try to fit, predict and score\n",
    "clf_2 = NB(vocabulary_2)\n",
    "scores = cross_val_score(clf_2, X_2, y, cv=5)\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La performance du prédicteur ne s'est pas améliorée avec la suppression des stop words. La meilleure prévision est maintenant de 81.25%.\n",
    "En rehgardant la liste des stop words, ce n'est pas forcément étonnant: on y voit des éléments qui devraient être discriminants (awfully, appropriate, sensible, useful, well) et qui sont donc retirés de l'algorithme de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II Utilisation de scikitlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Comparer votre implémentation avec scikitlearn. On utilisera la classe CountVectorizer et un Pipeline. Vous expérimenterez en autorisant les mots et bigrammes ou en travaillant sur les sous-chaines de\n",
    "caractères (option analyzer='char')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81299999999999994"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Pipeline, no cross validation\n",
    "# We do the same first test\n",
    "# Only words with size > 2 for this implementation\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                     ('multinomialNB', MultinomialNB())])\n",
    "\n",
    "pipeline.fit(texts[::2], y[::2])\n",
    "\n",
    "test = pipeline.predict(texts[1::2])\n",
    "\n",
    "np.mean(test == y[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première implémentation, sans tuner aucun paramètre, et sans cross validation est équivalente à celle de notre prédicteur custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84099999999999997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Pipeline, no cross validation\n",
    "# We do the same first test\n",
    "# Allow Bigrams for this implementation\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('multinomialNB', MultinomialNB())])\n",
    "\n",
    "pipeline.fit(texts[::2], y[::2])\n",
    "\n",
    "test = pipeline.predict(texts[1::2])\n",
    "\n",
    "np.mean(test == y[1::2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La deuxième version qui autorise les bigrammes améliore le score (84%)\n",
    "- On améliore la compréhension des avis en interprétant également les suites de 2 mots\n",
    "- On n'est pas en situation d'overfitting car la prévision du testing set ne s'est pas effondrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8175  0.8375  0.8225  0.8425  0.8325]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('multinomialNB', MultinomialNB())])\n",
    "\n",
    "scores = cross_val_score(pipeline, texts, y, cv=5)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prédiction sur le testing set reste stable avec la cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Tester un autre algorithme de la librairie scikitlearn (ex : LinearSVC, LogisticRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8225  0.8525  0.8525  0.87    0.865 ]\n"
     ]
    }
   ],
   "source": [
    "# Test avec LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('logisticRegression', LogisticRegression())])\n",
    "\n",
    "scores = cross_val_score(pipeline, texts, y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conservant les bigrammes, on obtient une prédiction légèrement meilleure avec la régression logistique (87%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Utiliser la librairie NLTK afin de procéder à une racinisation (stemming). Vous utiliserez la classe SnowballStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.795   0.8125  0.8025  0.8325  0.7925]\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words, ngram_range=(1, 2))\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', stem_vectorizer),\n",
    "                     ('multinomialNB', MultinomialNB())])\n",
    "\n",
    "scores = cross_val_score(pipeline, texts, y, cv=5)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 14)\t2\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t3\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "['and', 'be', 'ceas', 'expir', 'gone', 'has', 'is', 'it', 'maker', 'meet', 'more', 'no', 'parrot', 'this', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(stem_vectorizer.fit_transform(['This parrot is no more. It has ceased to be. '\n",
    "                                     'It s expired and gone to meet its maker.']))\n",
    "print(stem_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas eu de gain en terme de prédiction avec l utilisation du stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Filtrer les mots par catégorie grammaticale (POS : Part Of Speech) et ne garder que les\n",
    "noms, les verbes, les adverbes et les adjectifs pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textsample = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
